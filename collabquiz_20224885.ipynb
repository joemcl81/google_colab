{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collabquiz_20224885.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joemcl81/google_colab/blob/main/collabquiz_20224885.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUg-IkfZqJuY"
      },
      "source": [
        "Joseph Mc Laughlin, 20224885\n",
        "\n",
        "Collaborative Quiz of The Week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4xV2NehqUEA"
      },
      "source": [
        "Q1: \n",
        "\n",
        "Using an example, explain the difference between Type I and Type II Errors in the context of Regular expressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2JcYyoSGbUQ"
      },
      "source": [
        "**Type 1 and Type 2 Errors**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhFmExocqIro"
      },
      "source": [
        "#Solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37f009zBqXIs"
      },
      "source": [
        "Q2:\n",
        "\n",
        "Count the number of Types and Tokens in the following sentence: “NLP is the art of analyzing and understanding human languages by machines.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5I8vHpnqZNg",
        "outputId": "32480773-ee4d-48dc-fc86-9821280bcdad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Solution\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "num_words = 50\n",
        "text = [\"NLP is the art of analyzing and understanding human languages by machines.\"]\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(text)\n",
        "word_index = tokenizer.word_index\n",
        " \n",
        "print(\"Word index:\\n\", word_index)\n",
        "print('There are %s tokens in the text.' % len(word_index.keys()))\n",
        "print('There are %s types in the text.' % len(word_index))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index:\n",
            " {'nlp': 1, 'is': 2, 'the': 3, 'art': 4, 'of': 5, 'analyzing': 6, 'and': 7, 'understanding': 8, 'human': 9, 'languages': 10, 'by': 11, 'machines': 12}\n",
            "There are 12 tokens in the text.\n",
            "There are 12 types in the text.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI6FluM6qZcF"
      },
      "source": [
        "Q3:\n",
        "\n",
        "Explain the difference between lemmatization and stemming and provide an example for each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Y1zND3WG4z"
      },
      "source": [
        "**What is Stemming?**\n",
        "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.\n",
        "\n",
        "In another word, there is one root word, but there are many variations of the same words. For example, the root word is \"wait\" and it's variations are \"waits, waiting and waited\". In the same way, with the help of Stemming, we can find the root word of any variations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcwADWYxqbUq",
        "outputId": "85d563ec-b814-42aa-a601-ed3b7f5f2e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps =PorterStemmer()\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(e_words)\n",
        "for w in e_words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(\"Stemming for {} is {}\".format(w,rootWord))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming for wait is wait\n",
            "Stemming for waiting is wait\n",
            "Stemming for waited is wait\n",
            "Stemming for waits is wait\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebFoobiHX4Ln"
      },
      "source": [
        "**Discussion of Output**\n",
        "\n",
        "Stemming is a data-preprocessing module. The English language has many variations of a single word, i.e. wait. These variations create ambiguity in machine learning training and prediction. To create a successful model, it's vital to filter such words and convert to the same type of sequenced data using stemming. Also, this is an important technique to get row data from a set of sentence and removal of redundant data also known as normalization.\n",
        "\n",
        "**What is Lemmatization?** \n",
        "\n",
        "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma. The NLTK Lemmatization method is based on WorldNet's built-in morph function. Text preprocessing includes both stemming as well as lemmatization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTtEncQHT5g8",
        "outputId": "02d4ded3-1c17-4d4f-fb91-18bfd9fdaa8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Lemming\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "\tprint(\"Lemma for {} is {}\".format(w, wl.lemmatize(w))) "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_yUCMYYCQl"
      },
      "source": [
        "**Discussion of output:**\n",
        "\n",
        "If you use stemming for studies and studying, output is same (studi) but lemmatizer provides different lemma for both tokens study for studies and studying for studying. So when we need to make feature set to train machine, it would be great if lemmatization is preferred.\n",
        "\n",
        "**Stemming vs Lemmatizer**\n",
        "\n",
        "Many people find the two terms confusing. Some treat these as same, but there is a difference between these both. Lemmatization is preferred over the former. The stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word. Lemmatizer minimizes text ambiguity. Example words like bicycle or bicycles are converted to base word bicycle. Basically, it will convert all words having the same meaning but different representation to their base form. It reduces the word density in the given text and helps in preparing the accurate features for training machine. Cleaner the data, the more intelligent and accurate your machine learning model, will be. Lemmatizerwill also saves memory as well as computational cost."
      ]
    }
  ]
}