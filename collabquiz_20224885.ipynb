{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collabquiz_20224885.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTWS5xSVIZh616L0wLM2Hc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joemcl81/google_colab/blob/main/collabquiz_20224885.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUg-IkfZqJuY"
      },
      "source": [
        "Joseph Mc Laughlin, 20224885\n",
        "\n",
        "Collaborative Quiz of The Week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4xV2NehqUEA"
      },
      "source": [
        "Q1: \n",
        "\n",
        "Using an example, explain the difference between Type I and Type II Errors in the context of Regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhFmExocqIro"
      },
      "source": [
        "#Solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37f009zBqXIs"
      },
      "source": [
        "Q2:\n",
        "\n",
        "Count the number of Types and Tokens in the following sentence: “NLP is the art of analyzing and understanding human languages by machines.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5I8vHpnqZNg",
        "outputId": "428a28b8-0e54-482f-9137-7d99f73fee9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Solution\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "num_words = 50\n",
        "text = [\"NLP is the art of analyzing and understanding human languages by machines.\"]\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(text)\n",
        "word_index = tokenizer.word_index\n",
        " \n",
        "print(\"Word index:\\n\", word_index)\n",
        "print('There are %s tokens.' % len(word_index))\n",
        "#print('There are %s types.' % len(word_index))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index:\n",
            " {'nlp': 1, 'is': 2, 'the': 3, 'art': 4, 'of': 5, 'analyzing': 6, 'and': 7, 'understanding': 8, 'human': 9, 'languages': 10, 'by': 11, 'machines': 12}\n",
            "There are 12 tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI6FluM6qZcF"
      },
      "source": [
        "Q3:\n",
        "\n",
        "Explain the difference between lemmatization and stemming and provide an example for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcwADWYxqbUq",
        "outputId": "d59782aa-59f9-4d81-f566-243886fe9ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps =PorterStemmer()\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(e_words)\n",
        "for w in e_words:\n",
        "    rootWord=ps.stem(w)\n",
        "    #print(rootWord)\n",
        "    print(\"Stemming for {} is {}\".format(w,rootWord))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming for wait is wait\n",
            "Stemming for waiting is wait\n",
            "Stemming for waited is wait\n",
            "Stemming for waits is wait\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra5c76xL8Vfx",
        "outputId": "8060a6a5-15b9-42bd-d9f6-55dabff27534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Lemmatization\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "word_data = [\"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"]\n",
        "tokenizer.fit_on_texts(word_data)\n",
        "for w in word_data:\n",
        "  print(w)\n",
        "  #lemwords=wl.lemmatize(w)\n",
        "  #print \"Actual: %s  Lemma: %s\" % (w,wordnet_lemmatizer.lemmatize(w))\n",
        "  rootWord=ps.stem(w)\n",
        "    #print(rootWord)\n",
        "  print(\"Stemming for {} is {}\".format(w,rootWord))\n",
        "\t  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\n",
            "Stemming for It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms is it originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing room\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}