{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collabquiz_20224885.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joemcl81/google_colab/blob/main/collabquiz_20224885.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUg-IkfZqJuY"
      },
      "source": [
        "Joseph Mc Laughlin, 20224885\n",
        "\n",
        "Collaborative Quiz of The Week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4xV2NehqUEA"
      },
      "source": [
        "Q1: \n",
        "\n",
        "Using an example, explain the difference between Type I and Type II Errors in the context of Regular expressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2JcYyoSGbUQ"
      },
      "source": [
        "Statistical hypothesis testing implies that no test is ever 100% certain: that’s because we rely on probabilities to experiment.\n",
        "\n",
        "When online marketers and scientists run hypothesis tests, both seek out statistically relevant results. This means that the results of their test have to be true within a range of probabilities (typically 95%). A statistically significant result cannot prove that a research hypothesis is correct (as this implies 100% certainty). Because a p-value is based on probabilities, there is always a chance of making an incorrect conclusion regarding accepting or rejecting the null hypothesis (H0).\n",
        "\n",
        "Even though hypothesis tests are meant to be reliable, there are two types of errors that can occur.\n",
        "\n",
        "**Type 1**\n",
        "\n",
        "Type 1 errors – often assimilated with false positives – happen in hypothesis testing when the null hypothesis is true but rejected. The null hypothesis is a general statement or default position that there is no relationship between two measured phenomena.\n",
        "\n",
        "Simply put, type 1 errors are “false positives” – they happen when the tester validates a statistically significant difference even though there isn’t one.\n",
        "\n",
        "**Type 2**\n",
        "\n",
        "\n",
        "If type 1 errors are commonly referred to as “false positives”, type 2 errors are referred to as “false negatives”. Type 2 errors happen when you inaccurately assume that no winner has been declared between a control version and a variation although there actually is a winner. In more statistically accurate terms, type 2 errors happen when the null hypothesis is false and you subsequently fail to reject it.\n",
        "\n",
        "If the probability of making a type 1 error is determined by “α”, the probability of a type 2 error is “β”. Beta depends on the power of the test (i.e the probability of not committing a type 2 error, which is equal to 1-β)\n",
        "\n",
        "**Type 1 and Type 2 Errors in Regex**\n",
        "\n",
        "The code below demonstrates how Type 1 and Type 2 errors can occur when using Regex. The aim is to use regex to match all instances of the word \"the\" in a text. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhFmExocqIro",
        "outputId": "14c74b5f-579f-45f7-c258-e6a5ba4b039a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Demonstration\n",
        "import re\n",
        "text = \"The other one there, the blithe one.\"\n",
        "\n",
        "x = re.findall(\"the\", text) #Misses capitalized examples\n",
        "y = re.findall(\"[tT]he\", text) #Incorrectly returns other, there and blithe\n",
        "z = re.findall(\"[^A-Za-z][tT]he[^A-Za-z]\", text) #Returns instances of 'the' that don't have a alphabetic element before or after instances\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'the', 'the', 'the']\n",
            "['The', 'the', 'the', 'the', 'the']\n",
            "[' the ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXazLfJRLXRf"
      },
      "source": [
        "This code demonstrates both Type 1 and Type 2 errors in Regex as well as providing a solution to these errors. y represents the Type 1 or False Positive errors, where Regex finds matches for instances of \"the\" which we are not looking for, other, there and blite. x represents Type 2 or False Negative errors, where Regex misses matches, The due to the capital letters. z represents a solution to this, by only matching instances of 'the' that either begin or end with a non-alphabetic character. \n",
        "\n",
        "These types of errors must always be dealt with when developing NLP solutions. There are two agtagonistic approaches to reduing the error rates, minimising false positives by increasing accuracy/precision and minimising false negatives by increasing coverage/recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37f009zBqXIs"
      },
      "source": [
        "Q2:\n",
        "\n",
        "Count the number of Types and Tokens in the following sentence: “NLP is the art of analyzing and understanding human languages by machines.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5I8vHpnqZNg",
        "outputId": "d6c37ad3-97f1-4244-a918-7be05827793b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Solution\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "num_words = 50\n",
        "text = [\"NLP is the art of analyzing and understanding human languages by machines.\"]\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(text)\n",
        "word_index = tokenizer.word_index\n",
        " \n",
        "print(\"Word index:\\n\", word_index)\n",
        "print('There are %s tokens in the text.' % len(word_index.keys()))\n",
        "print('There are %s types in the text.' % len(word_index))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index:\n",
            " {'nlp': 1, 'is': 2, 'the': 3, 'art': 4, 'of': 5, 'analyzing': 6, 'and': 7, 'understanding': 8, 'human': 9, 'languages': 10, 'by': 11, 'machines': 12}\n",
            "There are 12 tokens in the text.\n",
            "There are 12 types in the text.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI6FluM6qZcF"
      },
      "source": [
        "Q3:\n",
        "\n",
        "Explain the difference between lemmatization and stemming and provide an example for each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Y1zND3WG4z"
      },
      "source": [
        "**What is Stemming?**\n",
        "\n",
        "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.\n",
        "\n",
        "In another word, there is one root word, but there are many variations of the same words. For example, the root word is \"wait\" and it's variations are \"waits, waiting and waited[link text](https://)\". In the same way, with the help of Stemming, we can find the root word of any variations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcwADWYxqbUq",
        "outputId": "1e808da2-8cda-48db-dc3c-7a235d1ca47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps =PorterStemmer()\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(e_words)\n",
        "for w in e_words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(\"Stemming for {} is {}\".format(w,rootWord))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming for wait is wait\n",
            "Stemming for waiting is wait\n",
            "Stemming for waited is wait\n",
            "Stemming for waits is wait\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebFoobiHX4Ln"
      },
      "source": [
        "**Discussion of Output:**\n",
        "\n",
        "Stemming is a data-preprocessing module. The English language has many variations of a single word, i.e. wait. These variations create ambiguity in machine learning training and prediction. To create a successful model, it's vital to filter such words and convert to the same type of sequenced data using stemming. Also, this is an important technique to get row data from a set of sentence and removal of redundant data also known as normalization.\n",
        "\n",
        "**What is Lemmatization?** \n",
        "\n",
        "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma. The NLTK Lemmatization method is based on WorldNet's built-in morph function. Text preprocessing includes both stemming as well as lemmatization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTtEncQHT5g8",
        "outputId": "47ab02ef-58ba-409f-e535-f86caa40c51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Lemming\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "\tprint(\"Lemma for {} is {}\".format(w, wl.lemmatize(w))) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_yUCMYYCQl"
      },
      "source": [
        "**Discussion of output:**\n",
        "\n",
        "If you use stemming for studies and studying, output is same (studi) but lemmatizer provides different lemma for both tokens study for studies and studying for studying. So when we need to make feature set to train machine, it would be great if lemmatization is preferred.\n",
        "\n",
        "**Stemming vs Lemmatizer**\n",
        "\n",
        "Many people find the two terms confusing. Some treat these as same, but there is a difference between these both. Lemmatization is preferred over the former. The stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word. Lemmatizer minimizes text ambiguity. Example words like bicycle or bicycles are converted to base word bicycle. Basically, it will convert all words having the same meaning but different representation to their base form. It reduces the word density in the given text and helps in preparing the accurate features for training machine. Cleaner the data, the more intelligent and accurate your machine learning model, will be. Lemmatizerwill also saves memory as well as computational cost."
      ]
    }
  ]
}